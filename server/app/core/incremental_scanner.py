import hashlib
import json
import logging
from pathlib import Path
from typing import Dict, Set, List, Optional
from datetime import datetime

from app.models.issue import IssueCandidate

logger = logging.getLogger("app.incremental_scanner")

class IncrementalScanner:
    """
    Incremental scanning with file change detection.
    Only rescans files that have changed since last scan.
    Provides 10-100x speedup on subsequent scans.
    """
    
    def __init__(self, project_path: str):
        self.project_path = Path(project_path)
        self.cache_file = self.project_path / ".vulnora_cache.json"
        self.cache = self._load_cache()
    
    def _load_cache(self) -> Dict:
        """Load cached scan results and file hashes"""
        if self.cache_file.exists():
            try:
                with open(self.cache_file, 'r') as f:
                    cache = json.load(f)
                    logger.info(f"âœ… Loaded cache with {len(cache.get('file_hashes', {}))} file hashes")
                    return cache
            except Exception as e:
                logger.warning(f"âš ï¸  Failed to load cache: {e}. Starting fresh.")
                return self._empty_cache()
        else:
            logger.info("ðŸ“ No cache found. This is the first scan.")
            return self._empty_cache()
    
    def _empty_cache(self) -> Dict:
        """Create empty cache structure"""
        return {
            "version": "1.0",
            "last_scan": None,
            "file_hashes": {},
            "scan_results": {},
            "metadata": {
                "total_scans": 0,
                "last_scan_duration": 0
            }
        }
    
    def _save_cache(self):
        """Persist cache to disk"""
        try:
            self.cache["last_scan"] = datetime.now().isoformat()
            self.cache["metadata"]["total_scans"] += 1
            
            with open(self.cache_file, 'w') as f:
                json.dump(self.cache, f, indent=2)
            
            logger.info(f"ðŸ’¾ Cache saved to {self.cache_file}")
        except Exception as e:
            logger.error(f"âŒ Failed to save cache: {e}")
    
    def _file_hash(self, file_path: Path) -> str:
        """Compute SHA256 hash of file content"""
        try:
            with open(file_path, 'rb') as f:
                return hashlib.sha256(f.read()).hexdigest()
        except Exception as e:
            logger.warning(f"âš ï¸  Failed to hash {file_path}: {e}")
            return ""
    
    def get_changed_files(self, all_files: List[str]) -> Set[str]:
        """
        Compare current files with cached hashes.
        Returns set of files that have changed or are new.
        """
        changed = set()
        unchanged = set()
        new_files = set()
        
        for file_path_str in all_files:
            file_path = Path(file_path_str)
            
            if not file_path.exists():
                continue
            
            current_hash = self._file_hash(file_path)
            cached_hash = self.cache["file_hashes"].get(file_path_str)
            
            if cached_hash is None:
                # New file
                new_files.add(file_path_str)
                changed.add(file_path_str)
            elif current_hash != cached_hash:
                # Modified file
                changed.add(file_path_str)
            else:
                # Unchanged file
                unchanged.add(file_path_str)
            
            # Update hash in cache
            self.cache["file_hashes"][file_path_str] = current_hash
        
        # Log statistics
        logger.info(f"ðŸ“Š Incremental scan analysis:")
        logger.info(f"   - Total files: {len(all_files)}")
        logger.info(f"   - New files: {len(new_files)}")
        logger.info(f"   - Changed files: {len(changed) - len(new_files)}")
        logger.info(f"   - Unchanged files: {len(unchanged)}")
        logger.info(f"   - Files to scan: {len(changed)}")
        
        if unchanged:
            logger.info(f"âš¡ Skipping {len(unchanged)} unchanged files (using cached results)")
        
        return changed
    
    def scan_incremental(
        self, 
        scanner,
        force_full_scan: bool = False
    ) -> List[IssueCandidate]:
        """
        Perform incremental scan:
        1. Discover all files
        2. Identify changed files
        3. Scan only changed files
        4. Merge with cached results
        5. Update cache
        """
        # Discover all files
        all_files = scanner.discover_files()
        
        if force_full_scan:
            logger.info("ðŸ”„ Force full scan requested - scanning all files")
            files_to_scan = set(all_files)
        else:
            # Get only changed files
            files_to_scan = self.get_changed_files(all_files)
        
        if not files_to_scan:
            logger.info("âœ¨ No changes detected - using cached results")
            # Still increment scan counter for statistics
            self._save_cache()
            return self._get_cached_results()
        
        # Scan only changed files
        logger.info(f"ðŸš€ Scanning {len(files_to_scan)} files...")
        new_results = {}
        
        # Temporarily replace scanner's file list with changed files only
        original_files = scanner.files_to_scan
        scanner.files_to_scan = list(files_to_scan)
        
        # Run the scan
        issues = scanner.scan() if hasattr(scanner, 'scan') else []
        
        # Restore original file list
        scanner.files_to_scan = original_files
        
        # Group issues by file
        for issue in issues:
            if issue.file_path not in new_results:
                new_results[issue.file_path] = []
            new_results[issue.file_path].append(self._serialize_issue(issue))
        
        # Merge with cached results (remove stale entries for changed files)
        for file_path in files_to_scan:
            if file_path in new_results:
                self.cache["scan_results"][file_path] = new_results[file_path]
            else:
                # File was scanned but no issues found
                self.cache["scan_results"][file_path] = []
        
        # Remove entries for deleted files
        all_files_set = set(all_files)
        deleted_files = set(self.cache["scan_results"].keys()) - all_files_set
        for deleted_file in deleted_files:
            del self.cache["scan_results"][deleted_file]
            if deleted_file in self.cache["file_hashes"]:
                del self.cache["file_hashes"][deleted_file]
        
        if deleted_files:
            logger.info(f"ðŸ—‘ï¸  Removed {len(deleted_files)} deleted files from cache")
        
        # Save cache
        self._save_cache()
        
        # Return all issues (cached + new)
        return self._get_all_results()
    
    def _serialize_issue(self, issue: IssueCandidate) -> Dict:
        """Convert IssueCandidate to JSON-serializable dict"""
        return {
            "file_path": issue.file_path,
            "line_number": issue.line_number,
            "column": issue.column,
            "rule_id": issue.rule_id,
            "vulnerability_type": issue.vulnerability_type,
            "severity": issue.severity,
            "description": issue.description,
            "confidence": issue.confidence,
            "snippet": issue.snippet,
            "suggested_fix": issue.suggested_fix,
            "fix_theory": issue.fix_theory
        }
    
    def _deserialize_issue(self, issue_dict: Dict) -> IssueCandidate:
        """Convert dict back to IssueCandidate"""
        return IssueCandidate(**issue_dict)
    
    def _get_cached_results(self) -> List[IssueCandidate]:
        """Get all cached scan results"""
        all_issues = []
        for file_issues in self.cache["scan_results"].values():
            for issue_dict in file_issues:
                all_issues.append(self._deserialize_issue(issue_dict))
        return all_issues
    
    def _get_all_results(self) -> List[IssueCandidate]:
        """Get all results (same as cached results for now)"""
        return self._get_cached_results()
    
    def clear_cache(self):
        """Clear all cached data"""
        if self.cache_file.exists():
            self.cache_file.unlink()
            logger.info("ðŸ—‘ï¸  Cache cleared")
        self.cache = self._empty_cache()
    
    def get_cache_stats(self) -> Dict:
        """Get cache statistics"""
        return {
            "cache_exists": self.cache_file.exists(),
            "last_scan": self.cache.get("last_scan"),
            "total_scans": self.cache["metadata"]["total_scans"],
            "cached_files": len(self.cache["file_hashes"]),
            "cached_issues": sum(len(issues) for issues in self.cache["scan_results"].values()),
            "cache_size_kb": self.cache_file.stat().st_size / 1024 if self.cache_file.exists() else 0
        }
